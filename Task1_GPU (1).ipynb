{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f226aa3772bb4baebb9eaf087738af25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b706e2a49a0a47939f25e9c7e7611ce0",
              "IPY_MODEL_6a62310fffb74bd3970ce999a582fa0b",
              "IPY_MODEL_cee428dd923844afbfc36905fbd9f32d"
            ],
            "layout": "IPY_MODEL_7f55392874524f5fb65f4de9dd281c55"
          }
        },
        "b706e2a49a0a47939f25e9c7e7611ce0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bf1fac358fc45f4b1a19c334ee86faa",
            "placeholder": "​",
            "style": "IPY_MODEL_fd6a7f7f55514ea88494c4da737a1073",
            "value": "model.safetensors: 100%"
          }
        },
        "6a62310fffb74bd3970ce999a582fa0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7b0171b0d34463aabbde9b98e8d1e3b",
            "max": 200928946,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86fd9c11a9844e2594b397e179f8cf94",
            "value": 200928946
          }
        },
        "cee428dd923844afbfc36905fbd9f32d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f38e8ce1eb1a4c5d94370457c4c169c8",
            "placeholder": "​",
            "style": "IPY_MODEL_4f1471b4173b4a3792f65aedfafa91bb",
            "value": " 201M/201M [00:01&lt;00:00, 134MB/s]"
          }
        },
        "7f55392874524f5fb65f4de9dd281c55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bf1fac358fc45f4b1a19c334ee86faa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd6a7f7f55514ea88494c4da737a1073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7b0171b0d34463aabbde9b98e8d1e3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86fd9c11a9844e2594b397e179f8cf94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f38e8ce1eb1a4c5d94370457c4c169c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f1471b4173b4a3792f65aedfafa91bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Using EfficientB7 as feature extractor for task 1**\n"
      ],
      "metadata": {
        "id": "djTHbZvUQXbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import pickle\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Load EfficientNet-B7 model\n",
        "model = models.efficientnet_b7(pretrained=True)\n",
        "feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n",
        "feature_extractor.eval()  # Set to evaluation mode\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),  # Convert tensor to PIL image\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),  # Convert PIL image back to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
        "])\n",
        "\n",
        "class UnlabeledDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images, transform):\n",
        "        self.images = images\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "class LWPClassifier:\n",
        "    def __init__(self, num_classes=2):\n",
        "        self.num_classes = num_classes\n",
        "        self.prototypes = None  # Prototypes will be initialized during training\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Initialize prototypes as the mean of features for each class\n",
        "        self.prototypes = np.array([X[y == c].mean(axis=0) for c in range(self.num_classes)])\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Compute distances to prototypes\n",
        "        distances = np.linalg.norm(X[:, np.newaxis] - self.prototypes, axis=2)  # Shape: [num_samples, num_classes]\n",
        "        return np.argmin(distances, axis=1)  # Assign to the nearest prototype\n",
        "\n",
        "def get_features(images, transform):\n",
        "    dataset = UnlabeledDataset(images, transform)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    feature_extractor.eval()\n",
        "    features = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_images in loader:\n",
        "            batch_images = batch_images.to(device)\n",
        "            batch_features = feature_extractor(batch_images)  # Shape: [batch_size, 2560, 7, 7]\n",
        "            batch_features = batch_features.mean([2, 3])  # Global Average Pooling: [batch_size, 2560]\n",
        "            features.append(batch_features.cpu())\n",
        "\n",
        "    # Combine features into a single tensor\n",
        "    features = torch.cat(features)\n",
        "    return features\n",
        "\n",
        "all_eval_features = []\n",
        "all_eval_labels = []\n",
        "acc_matrix = np.zeros((10, 10))\n",
        "prev_feature = []\n",
        "prev_labels = []\n",
        "data_paths = [f\"/content/drive/MyDrive/train_data/{i}_train_data.tar.pth\" for i in range(1, 11)]\n",
        "heldout_paths = [f\"/content/drive/MyDrive/eval_data/{i}_eval_data.tar.pth\" for i in range(1, 11)]\n",
        "\n",
        "for loop in range(10):\n",
        "    data = torch.load(data_paths[loop])\n",
        "    heldout = torch.load(heldout_paths[loop])\n",
        "\n",
        "    train_images = data['data']\n",
        "    train_labels = data['targets'] if loop == 0 else prev_labels\n",
        "    eval_images = heldout['data']\n",
        "    eval_labels = heldout['targets']\n",
        "\n",
        "    feature_extractor = feature_extractor.to(device)\n",
        "\n",
        "    train_features = get_features(train_images, transform) if loop == 0 else prev_feature\n",
        "    print(f\"{loop} Done training features\")\n",
        "\n",
        "    f = LWPClassifier(num_classes=10)\n",
        "    f.fit(train_features, train_labels)\n",
        "\n",
        "    if loop == 9:\n",
        "        with open(\"lwp_classifier.pkl\", \"wb\") as file:\n",
        "            pickle.dump(f, file)\n",
        "\n",
        "    eval_features = get_features(eval_images, transform)\n",
        "    print(f\"{loop} Done eval features\")\n",
        "    all_eval_features.append(eval_features)\n",
        "    all_eval_labels.append(eval_labels)\n",
        "\n",
        "    accuracies = []\n",
        "    for j in range(loop + 1):\n",
        "        preds = f.predict(all_eval_features[j])\n",
        "        acc = accuracy_score(all_eval_labels[j], preds)\n",
        "        accuracies.append(acc)\n",
        "        acc_matrix[loop][j] = acc\n",
        "\n",
        "    pred_labels = f.predict(eval_features)\n",
        "    acc = accuracy_score(eval_labels, pred_labels)\n",
        "    accuracies.append(acc)\n",
        "    acc_matrix[loop][loop] = acc\n",
        "    print(f\"{loop} Test Accuracy: {accuracies}\")\n",
        "\n",
        "    if loop != 9:\n",
        "        next_data = torch.load(data_paths[loop + 1])\n",
        "        next_images = next_data['data']\n",
        "        next_features = get_features(next_images, transform)\n",
        "        print(f\"{loop} Done next features\")\n",
        "        prev_feature = next_features\n",
        "        prev_labels = f.predict(next_features)\n",
        "\n",
        "print(acc_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl4IbbO-77Xu",
        "outputId": "8544d5bd-8903-48f8-c54e-e567b74c794a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "0 Done training features\n",
            "0 Done eval features\n",
            "0 Test Accuracy: [0.8504, 0.8504]\n",
            "0 Done next features\n",
            "1 Done training features\n",
            "1 Done eval features\n",
            "1 Test Accuracy: [0.8264, 0.8564, 0.8564]\n",
            "1 Done next features\n",
            "2 Done training features\n",
            "2 Done eval features\n",
            "2 Test Accuracy: [0.8188, 0.8484, 0.8332, 0.8332]\n",
            "2 Done next features\n",
            "3 Done training features\n",
            "3 Done eval features\n",
            "3 Test Accuracy: [0.818, 0.8484, 0.8308, 0.8388, 0.8388]\n",
            "3 Done next features\n",
            "4 Done training features\n",
            "4 Done eval features\n",
            "4 Test Accuracy: [0.8144, 0.8416, 0.826, 0.8348, 0.8428, 0.8428]\n",
            "4 Done next features\n",
            "5 Done training features\n",
            "5 Done eval features\n",
            "5 Test Accuracy: [0.8104, 0.8368, 0.822, 0.8288, 0.8312, 0.8208, 0.8208]\n",
            "5 Done next features\n",
            "6 Done training features\n",
            "6 Done eval features\n",
            "6 Test Accuracy: [0.814, 0.8368, 0.8192, 0.832, 0.8344, 0.8208, 0.8252, 0.8252]\n",
            "6 Done next features\n",
            "7 Done training features\n",
            "7 Done eval features\n",
            "7 Test Accuracy: [0.8048, 0.826, 0.812, 0.8208, 0.8244, 0.8104, 0.8144, 0.8292, 0.8292]\n",
            "7 Done next features\n",
            "8 Done training features\n",
            "8 Done eval features\n",
            "8 Test Accuracy: [0.7948, 0.816, 0.8004, 0.8092, 0.8156, 0.8016, 0.8068, 0.8156, 0.8068, 0.8068]\n",
            "8 Done next features\n",
            "9 Done training features\n",
            "9 Done eval features\n",
            "9 Test Accuracy: [0.792, 0.8092, 0.7968, 0.8024, 0.8072, 0.8, 0.8012, 0.8108, 0.8008, 0.8096, 0.8096]\n",
            "[[0.8504 0.     0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
            " [0.8264 0.8564 0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
            " [0.8188 0.8484 0.8332 0.     0.     0.     0.     0.     0.     0.    ]\n",
            " [0.818  0.8484 0.8308 0.8388 0.     0.     0.     0.     0.     0.    ]\n",
            " [0.8144 0.8416 0.826  0.8348 0.8428 0.     0.     0.     0.     0.    ]\n",
            " [0.8104 0.8368 0.822  0.8288 0.8312 0.8208 0.     0.     0.     0.    ]\n",
            " [0.814  0.8368 0.8192 0.832  0.8344 0.8208 0.8252 0.     0.     0.    ]\n",
            " [0.8048 0.826  0.812  0.8208 0.8244 0.8104 0.8144 0.8292 0.     0.    ]\n",
            " [0.7948 0.816  0.8004 0.8092 0.8156 0.8016 0.8068 0.8156 0.8068 0.    ]\n",
            " [0.792  0.8092 0.7968 0.8024 0.8072 0.8    0.8012 0.8108 0.8008 0.8096]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using EfficientB7 as feature extractor for task 2**"
      ],
      "metadata": {
        "id": "4pg7JACVQvbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load the classifier from the file\n",
        "with open(\"lwp_classifier.pkl\", \"rb\") as file:\n",
        "    f = pickle.load(file)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "batch_size = 16\n",
        "\n",
        "# Load EfficientNet-B7 as the feature extractor\n",
        "model = models.efficientnet_b7(pretrained=True)\n",
        "feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])  # Remove the classification head\n",
        "feature_extractor.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),  # Convert tensor to PIL image\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224 (EfficientNet-B7 default input size)\n",
        "    transforms.ToTensor(),  # Convert PIL image back to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
        "])\n",
        "\n",
        "class UnlabeledDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images, transform):  # Corrected __init__\n",
        "        self.images = images\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):  # Corrected __len__\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):  # Corrected __getitem__\n",
        "        img = self.images[idx]\n",
        "        img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "def get_features(images, transform):\n",
        "    dataset = UnlabeledDataset(images, transform)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    feature_extractor.eval()\n",
        "    features = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_images in loader:\n",
        "            batch_images = batch_images.to(device)\n",
        "            batch_features = feature_extractor(batch_images)  # Extract features\n",
        "            batch_features = batch_features.mean([2, 3])  # Global Average Pooling\n",
        "            features.append(batch_features.cpu())\n",
        "\n",
        "    # Combine features into a single tensor\n",
        "    features = torch.cat(features)\n",
        "    return features\n",
        "\n",
        "# Initialize variables\n",
        "all_eval_features = []\n",
        "all_eval_labels = []\n",
        "acc_matrix = np.zeros((10, 10))\n",
        "data_paths = [f\"/content/drive/MyDrive/train_data02/{i}_train_data.tar.pth\" for i in range(1, 11)]\n",
        "heldout_paths = [f\"/content/drive/MyDrive/eval_data02/{i}_eval_data.tar.pth\" for i in range(1, 11)]\n",
        "\n",
        "# Main training and evaluation loop\n",
        "for loop in range(10):\n",
        "    data = torch.load(data_paths[loop])\n",
        "    heldout = torch.load(heldout_paths[loop])\n",
        "\n",
        "    train_images = data['data']\n",
        "    eval_images = heldout['data']\n",
        "    eval_labels = heldout['targets']\n",
        "\n",
        "    feature_extractor = feature_extractor.to(device)\n",
        "\n",
        "    # Extract training features\n",
        "    train_features = get_features(train_images, transform)\n",
        "    print(f\"{loop} Done training features\")\n",
        "    train_labels = f.predict(train_features)\n",
        "\n",
        "    # Fit the classifier\n",
        "    f.fit(train_features, train_labels)\n",
        "\n",
        "    # Extract evaluation features\n",
        "    eval_features = get_features(eval_images, transform)\n",
        "    print(f\"{loop} Done eval features\")\n",
        "    all_eval_features.append(eval_features)\n",
        "    all_eval_labels.append(eval_labels)\n",
        "\n",
        "    # Compute accuracies\n",
        "    accuracies = []\n",
        "    for j in range(loop):\n",
        "        preds = f.predict(all_eval_features[j])\n",
        "        acc = accuracy_score(all_eval_labels[j], preds)\n",
        "        accuracies.append(acc)\n",
        "        acc_matrix[loop][j] = acc\n",
        "\n",
        "    pred_labels = f.predict(eval_features)\n",
        "    acc = accuracy_score(eval_labels, pred_labels)\n",
        "    accuracies.append(acc)\n",
        "    acc_matrix[loop][loop] = acc\n",
        "    print(f\"{loop} Test Accuracy: {accuracies}\")\n",
        "\n",
        "# Print the accuracy matrix\n",
        "print(acc_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3lyyRiV_xhK",
        "outputId": "369173e1-c372-4b3a-999f-4505ac5a4116"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "0 Done training features\n",
            "0 Done eval features\n",
            "0 Test Accuracy: [0.6348]\n",
            "1 Done training features\n",
            "1 Done eval features\n",
            "1 Test Accuracy: [0.5524, 0.3552]\n",
            "2 Done training features\n",
            "2 Done eval features\n",
            "2 Test Accuracy: [0.596, 0.3508, 0.6244]\n",
            "3 Done training features\n",
            "3 Done eval features\n",
            "3 Test Accuracy: [0.608, 0.3468, 0.6384, 0.6984]\n",
            "4 Done training features\n",
            "4 Done eval features\n",
            "4 Test Accuracy: [0.636, 0.3568, 0.6496, 0.7216, 0.7648]\n",
            "5 Done training features\n",
            "5 Done eval features\n",
            "5 Test Accuracy: [0.6088, 0.3396, 0.634, 0.6952, 0.7364, 0.6028]\n",
            "6 Done training features\n",
            "6 Done eval features\n",
            "6 Test Accuracy: [0.6152, 0.3312, 0.6336, 0.6956, 0.7432, 0.5996, 0.6816]\n",
            "7 Done training features\n",
            "7 Done eval features\n",
            "7 Test Accuracy: [0.6024, 0.3376, 0.6288, 0.6924, 0.728, 0.5944, 0.6676, 0.5992]\n",
            "8 Done training features\n",
            "8 Done eval features\n",
            "8 Test Accuracy: [0.5944, 0.3416, 0.6224, 0.682, 0.7128, 0.5892, 0.656, 0.5892, 0.5836]\n",
            "9 Done training features\n",
            "9 Done eval features\n",
            "9 Test Accuracy: [0.6164, 0.3472, 0.6344, 0.702, 0.7416, 0.602, 0.68, 0.5996, 0.5932, 0.7168]\n",
            "[[0.6348 0.     0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
            " [0.5524 0.3552 0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
            " [0.596  0.3508 0.6244 0.     0.     0.     0.     0.     0.     0.    ]\n",
            " [0.608  0.3468 0.6384 0.6984 0.     0.     0.     0.     0.     0.    ]\n",
            " [0.636  0.3568 0.6496 0.7216 0.7648 0.     0.     0.     0.     0.    ]\n",
            " [0.6088 0.3396 0.634  0.6952 0.7364 0.6028 0.     0.     0.     0.    ]\n",
            " [0.6152 0.3312 0.6336 0.6956 0.7432 0.5996 0.6816 0.     0.     0.    ]\n",
            " [0.6024 0.3376 0.6288 0.6924 0.728  0.5944 0.6676 0.5992 0.     0.    ]\n",
            " [0.5944 0.3416 0.6224 0.682  0.7128 0.5892 0.656  0.5892 0.5836 0.    ]\n",
            " [0.6164 0.3472 0.6344 0.702  0.7416 0.602  0.68   0.5996 0.5932 0.7168]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using ConvNext as feature extractor for task 1**\n"
      ],
      "metadata": {
        "id": "a3gzd14NKl8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import pickle\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import timm  # For ConvNeXt models\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Load ConvNeXt model\n",
        "model = timm.create_model('convnext_small', pretrained=True)\n",
        "feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])  # Remove classifier head\n",
        "feature_extractor.eval()  # Set to evaluation mode\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),  # Convert tensor to PIL image\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),  # Convert PIL image back to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
        "])\n",
        "\n",
        "class UnlabeledDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images, transform):\n",
        "        self.images = images\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "class LWPClassifier:\n",
        "    def __init__(self, num_classes=2):\n",
        "        self.num_classes = num_classes\n",
        "        self.prototypes = None  # Prototypes will be initialized during training\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Initialize prototypes as the mean of features for each class\n",
        "        self.prototypes = np.array([X[y == c].mean(axis=0) for c in range(self.num_classes)])\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Compute distances to prototypes\n",
        "        distances = np.linalg.norm(X[:, np.newaxis] - self.prototypes, axis=2)  # Shape: [num_samples, num_classes]\n",
        "        return np.argmin(distances, axis=1)  # Assign to the nearest prototype\n",
        "\n",
        "def get_features(images, transform):\n",
        "    dataset = UnlabeledDataset(images, transform)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    feature_extractor.eval()\n",
        "    features = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_images in loader:\n",
        "            batch_images = batch_images.to(device)\n",
        "            batch_features = feature_extractor(batch_images)  # Shape: [batch_size, feature_dim, H, W]\n",
        "            batch_features = batch_features.mean([2, 3])  # Global Average Pooling\n",
        "            features.append(batch_features.cpu())\n",
        "\n",
        "    # Combine features into a single tensor\n",
        "    features = torch.cat(features)\n",
        "    return features\n",
        "\n",
        "all_eval_features = []\n",
        "all_eval_labels = []\n",
        "acc_matrix = np.zeros((10, 10))\n",
        "prev_feature = []\n",
        "prev_labels = []\n",
        "data_paths = [f\"/content/drive/MyDrive/train_data/{i}_train_data.tar.pth\" for i in range(1, 11)]\n",
        "heldout_paths = [f\"/content/drive/MyDrive/eval_data/{i}_eval_data.tar.pth\" for i in range(1, 11)]\n",
        "\n",
        "for loop in range(10):\n",
        "    data = torch.load(data_paths[loop])\n",
        "    heldout = torch.load(heldout_paths[loop])\n",
        "\n",
        "    train_images = data['data']\n",
        "    train_labels = data['targets'] if loop == 0 else prev_labels\n",
        "    eval_images = heldout['data']\n",
        "    eval_labels = heldout['targets']\n",
        "\n",
        "    feature_extractor = feature_extractor.to(device)\n",
        "\n",
        "    train_features = get_features(train_images, transform) if loop == 0 else prev_feature\n",
        "    print(f\"{loop} Done training features\")\n",
        "\n",
        "    f = LWPClassifier(num_classes=10)\n",
        "    f.fit(train_features, train_labels)\n",
        "\n",
        "    if loop == 9:\n",
        "        with open(\"lwp_classifier.pkl\", \"wb\") as file:\n",
        "            pickle.dump(f, file)\n",
        "\n",
        "    eval_features = get_features(eval_images, transform)\n",
        "    print(f\"{loop} Done eval features\")\n",
        "    all_eval_features.append(eval_features)\n",
        "    all_eval_labels.append(eval_labels)\n",
        "\n",
        "    accuracies = []\n",
        "    for j in range(loop + 1):\n",
        "        preds = f.predict(all_eval_features[j])\n",
        "        acc = accuracy_score(all_eval_labels[j], preds)\n",
        "        accuracies.append(acc)\n",
        "        acc_matrix[loop][j] = acc\n",
        "\n",
        "    pred_labels = f.predict(eval_features)\n",
        "    acc = accuracy_score(eval_labels, pred_labels)\n",
        "    accuracies.append(acc)\n",
        "    acc_matrix[loop][loop] = acc\n",
        "    print(f\"{loop} Test Accuracy: {accuracies}\")\n",
        "\n",
        "    if loop != 9:\n",
        "        next_data = torch.load(data_paths[loop + 1])\n",
        "        next_images = next_data['data']\n",
        "        next_features = get_features(next_images, transform)\n",
        "        print(f\"{loop} Done next features\")\n",
        "        prev_feature = next_features\n",
        "        prev_labels = f.predict(next_features)\n",
        "\n",
        "print(acc_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917,
          "referenced_widgets": [
            "f226aa3772bb4baebb9eaf087738af25",
            "b706e2a49a0a47939f25e9c7e7611ce0",
            "6a62310fffb74bd3970ce999a582fa0b",
            "cee428dd923844afbfc36905fbd9f32d",
            "7f55392874524f5fb65f4de9dd281c55",
            "3bf1fac358fc45f4b1a19c334ee86faa",
            "fd6a7f7f55514ea88494c4da737a1073",
            "c7b0171b0d34463aabbde9b98e8d1e3b",
            "86fd9c11a9844e2594b397e179f8cf94",
            "f38e8ce1eb1a4c5d94370457c4c169c8",
            "4f1471b4173b4a3792f65aedfafa91bb"
          ]
        },
        "id": "BJu1e32vJtcZ",
        "outputId": "5a6dc171-ced5-4845-cf4d-8a037aa938ed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/201M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f226aa3772bb4baebb9eaf087738af25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Done training features\n",
            "0 Done eval features\n",
            "0 Test Accuracy: [0.9316, 0.9316]\n",
            "0 Done next features\n",
            "1 Done training features\n",
            "1 Done eval features\n",
            "1 Test Accuracy: [0.9268, 0.93, 0.93]\n",
            "1 Done next features\n",
            "2 Done training features\n",
            "2 Done eval features\n",
            "2 Test Accuracy: [0.924, 0.9256, 0.9304, 0.9304]\n",
            "2 Done next features\n",
            "3 Done training features\n",
            "3 Done eval features\n",
            "3 Test Accuracy: [0.924, 0.9252, 0.932, 0.9312, 0.9312]\n",
            "3 Done next features\n",
            "4 Done training features\n",
            "4 Done eval features\n",
            "4 Test Accuracy: [0.9228, 0.9284, 0.934, 0.9312, 0.934, 0.934]\n",
            "4 Done next features\n",
            "5 Done training features\n",
            "5 Done eval features\n",
            "5 Test Accuracy: [0.9216, 0.9248, 0.9296, 0.9248, 0.928, 0.9292, 0.9292]\n",
            "5 Done next features\n",
            "6 Done training features\n",
            "6 Done eval features\n",
            "6 Test Accuracy: [0.9144, 0.9204, 0.9232, 0.9228, 0.9216, 0.9224, 0.932, 0.932]\n",
            "6 Done next features\n",
            "7 Done training features\n",
            "7 Done eval features\n",
            "7 Test Accuracy: [0.918, 0.9196, 0.9232, 0.9216, 0.9244, 0.926, 0.9312, 0.9208, 0.9208]\n",
            "7 Done next features\n",
            "8 Done training features\n",
            "8 Done eval features\n",
            "8 Test Accuracy: [0.9208, 0.9192, 0.9236, 0.9232, 0.926, 0.9252, 0.9344, 0.9228, 0.924, 0.924]\n",
            "8 Done next features\n",
            "9 Done training features\n",
            "9 Done eval features\n",
            "9 Test Accuracy: [0.9172, 0.9172, 0.92, 0.918, 0.9196, 0.9192, 0.9284, 0.9176, 0.9164, 0.9272, 0.9272]\n",
            "[[0.9316 0.     0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
            " [0.9268 0.93   0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
            " [0.924  0.9256 0.9304 0.     0.     0.     0.     0.     0.     0.    ]\n",
            " [0.924  0.9252 0.932  0.9312 0.     0.     0.     0.     0.     0.    ]\n",
            " [0.9228 0.9284 0.934  0.9312 0.934  0.     0.     0.     0.     0.    ]\n",
            " [0.9216 0.9248 0.9296 0.9248 0.928  0.9292 0.     0.     0.     0.    ]\n",
            " [0.9144 0.9204 0.9232 0.9228 0.9216 0.9224 0.932  0.     0.     0.    ]\n",
            " [0.918  0.9196 0.9232 0.9216 0.9244 0.926  0.9312 0.9208 0.     0.    ]\n",
            " [0.9208 0.9192 0.9236 0.9232 0.926  0.9252 0.9344 0.9228 0.924  0.    ]\n",
            " [0.9172 0.9172 0.92   0.918  0.9196 0.9192 0.9284 0.9176 0.9164 0.9272]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using ConvNext as feature extractor for task 2**"
      ],
      "metadata": {
        "id": "UZfl6ImjRNkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load the classifier from the file\n",
        "with open(\"lwp_classifier.pkl\", \"rb\") as file:\n",
        "    f = pickle.load(file)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import timm  # For ConvNeXt models\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "batch_size = 16\n",
        "\n",
        "# Load ConvNeXt-Large as the feature extractor\n",
        "model = timm.create_model('convnext_small', pretrained=True)\n",
        "feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])  # Remove the classification head\n",
        "feature_extractor.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),  # Convert tensor to PIL image\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224 (ConvNeXt default input size)\n",
        "    transforms.ToTensor(),  # Convert PIL image back to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
        "])\n",
        "\n",
        "class UnlabeledDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images, transform):\n",
        "        self.images = images\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "def get_features(images, transform):\n",
        "    dataset = UnlabeledDataset(images, transform)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    feature_extractor.eval()\n",
        "    features = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_images in loader:\n",
        "            batch_images = batch_images.to(device)\n",
        "            batch_features = feature_extractor(batch_images)  # Extract features\n",
        "            batch_features = batch_features.mean([2, 3])  # Global Average Pooling\n",
        "            features.append(batch_features.cpu())\n",
        "\n",
        "    # Combine features into a single tensor\n",
        "    features = torch.cat(features)\n",
        "    return features\n",
        "\n",
        "# Initialize variables\n",
        "all_eval_features = []\n",
        "all_eval_labels = []\n",
        "acc_matrix = np.zeros((10, 10))\n",
        "data_paths = [f\"/content/drive/MyDrive/train_data02/{i}_train_data.tar.pth\" for i in range(1, 11)]\n",
        "heldout_paths = [f\"/content/drive/MyDrive/eval_data02/{i}_eval_data.tar.pth\" for i in range(1, 11)]\n",
        "\n",
        "# Main training and evaluation loop\n",
        "for loop in range(10):\n",
        "    data = torch.load(data_paths[loop])\n",
        "    heldout = torch.load(heldout_paths[loop])\n",
        "\n",
        "    train_images = data['data']\n",
        "    eval_images = heldout['data']\n",
        "    eval_labels = heldout['targets']\n",
        "\n",
        "    feature_extractor = feature_extractor.to(device)\n",
        "\n",
        "    # Extract training features\n",
        "    train_features = get_features(train_images, transform)\n",
        "    print(f\"{loop} Done training features\")\n",
        "    train_labels = f.predict(train_features)\n",
        "\n",
        "    # Fit the classifier\n",
        "    f.fit(train_features, train_labels)\n",
        "\n",
        "    # Extract evaluation features\n",
        "    eval_features = get_features(eval_images, transform)\n",
        "    print(f\"{loop} Done eval features\")\n",
        "    all_eval_features.append(eval_features)\n",
        "    all_eval_labels.append(eval_labels)\n",
        "\n",
        "    # Compute accuracies\n",
        "    accuracies = []\n",
        "    for j in range(loop):\n",
        "        preds = f.predict(all_eval_features[j])\n",
        "        acc = accuracy_score(all_eval_labels[j], preds)\n",
        "        accuracies.append(acc)\n",
        "        acc_matrix[loop][j] = acc\n",
        "\n",
        "    pred_labels = f.predict(eval_features)\n",
        "    acc = accuracy_score(eval_labels, pred_labels)\n",
        "    accuracies.append(acc)\n",
        "    acc_matrix[loop][loop] = acc\n",
        "    print(f\"{loop} Test Accuracy: {accuracies}\")\n",
        "\n",
        "# Print the accuracy matrix\n",
        "print(acc_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bY4awYrvOMOl",
        "outputId": "2e5f07f9-b9d0-4b52-b13f-6d09bc65d94d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "0 Done training features\n",
            "0 Done eval features\n",
            "0 Test Accuracy: [0.7684]\n",
            "1 Done training features\n",
            "1 Done eval features\n",
            "1 Test Accuracy: [0.7336, 0.6276]\n",
            "2 Done training features\n",
            "2 Done eval features\n",
            "2 Test Accuracy: [0.7476, 0.6064, 0.8192]\n",
            "3 Done training features\n",
            "3 Done eval features\n",
            "3 Test Accuracy: [0.7616, 0.6008, 0.8244, 0.9032]\n",
            "4 Done training features\n",
            "4 Done eval features\n",
            "4 Test Accuracy: [0.7508, 0.5928, 0.8232, 0.9036, 0.9056]\n",
            "5 Done training features\n",
            "5 Done eval features\n",
            "5 Test Accuracy: [0.7488, 0.5912, 0.8244, 0.8864, 0.8956, 0.7688]\n",
            "6 Done training features\n",
            "6 Done eval features\n",
            "6 Test Accuracy: [0.7416, 0.588, 0.8124, 0.8836, 0.8872, 0.7708, 0.8264]\n",
            "7 Done training features\n",
            "7 Done eval features\n",
            "7 Test Accuracy: [0.7464, 0.594, 0.8232, 0.88, 0.886, 0.7592, 0.8268, 0.7948]\n",
            "8 Done training features\n",
            "8 Done eval features\n",
            "8 Test Accuracy: [0.7236, 0.5752, 0.8, 0.8596, 0.866, 0.7384, 0.784, 0.7828, 0.7568]\n",
            "9 Done training features\n",
            "9 Done eval features\n",
            "9 Test Accuracy: [0.7408, 0.5756, 0.812, 0.8756, 0.8856, 0.7552, 0.8, 0.7944, 0.7424, 0.866]\n",
            "[[0.7684 0.     0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
            " [0.7336 0.6276 0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
            " [0.7476 0.6064 0.8192 0.     0.     0.     0.     0.     0.     0.    ]\n",
            " [0.7616 0.6008 0.8244 0.9032 0.     0.     0.     0.     0.     0.    ]\n",
            " [0.7508 0.5928 0.8232 0.9036 0.9056 0.     0.     0.     0.     0.    ]\n",
            " [0.7488 0.5912 0.8244 0.8864 0.8956 0.7688 0.     0.     0.     0.    ]\n",
            " [0.7416 0.588  0.8124 0.8836 0.8872 0.7708 0.8264 0.     0.     0.    ]\n",
            " [0.7464 0.594  0.8232 0.88   0.886  0.7592 0.8268 0.7948 0.     0.    ]\n",
            " [0.7236 0.5752 0.8    0.8596 0.866  0.7384 0.784  0.7828 0.7568 0.    ]\n",
            " [0.7408 0.5756 0.812  0.8756 0.8856 0.7552 0.8    0.7944 0.7424 0.866 ]]\n"
          ]
        }
      ]
    }
  ]
}